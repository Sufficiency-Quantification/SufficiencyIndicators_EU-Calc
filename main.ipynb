{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec7d9b",
   "metadata": {},
   "source": [
    "#### 1. Import Data\n",
    "\n",
    "We use data from the __bitbucket__ repository of EUcalc and Data downloaded from the EUcalc Website directly. At first we import the data on population, once the historical (__pop_hist__) and the the future population (__pop_life__). Also we load the levers defined for the _Life_ scenario to be able to calculate the correct values for each variable. \n",
    "\n",
    "The Levers hereby indicate the ambition level for a given category, meaning the higher the lever the higher the ambition in that category. The levers go from 1-4, where for each lever future values are calculated and available on the EUcalc repository. For some categories, like population, the Lever is a decimal number, so the correct value for that category has to be interpolated with the given levers. This is done in the __calculate_new_lever__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1c3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# historic population\n",
    "pop_hist = pd.read_csv('input/eu_calc_repo_data/ots_lfs_pop.csv').drop([\"iiasa_ssp2\"], axis=1).rename(columns={\"ots_lfs_pop_population[inhabitants]\":\"population\"})\n",
    "\n",
    "# get all countries listed in the data\n",
    "countries = pop_hist['Country'].unique()\n",
    "\n",
    "# population in the life scenario\n",
    "pop_life = pd.read_csv(\"input/eu_calc_repo_data/fts_lfs_pop.csv\").drop(\"iiasa_ssp2\", axis=1).rename(columns={\"fts_lfs_pop_population[inhabitants]\":\"population\", \"lever_pop\":\"lever\"})\n",
    "\n",
    "# Levers per Category of LIFE Scenario\n",
    "levers_life = pd.read_json('input/levers_life.json').T\n",
    "\n",
    "# Change the column name to lever for accessability\n",
    "levers_life.columns = ['lever']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90e0a0",
   "metadata": {},
   "source": [
    "### 2. Process Population Data\n",
    "\n",
    "#### 2.1 Calculate decimal levers\n",
    "\n",
    "For some categories, like the population there is no direct lever but a decimal lever, so we need to interpolate. Therefore we give the function the data, the wanted column in that data, the lever value and the column name where the lever value can be found. Then we interpolate to get the value for the given decimal lever and return a Dataframe with the lever_value, Years and Countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97058f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the lever for a given column and Dataframe. Is used when lever is decimal e.g. 1.5\n",
    "def calculate_new_lever(data, column, lever, lever_column='lever'):\n",
    "    '''\n",
    "    Calculates the lever for a given column and Dataframe. Is used when lever is decimal e.g. 1.5\n",
    "    :param data: Dataframe with the data\n",
    "    :param column: Column name for which the lever should be calculated\n",
    "    :param lever: Lever value\n",
    "    :param lever_column: Column name where the value value is found\n",
    "    :return: Dataframe with the calculated lever\n",
    "    '''\n",
    "    # Get lower and upper lever\n",
    "    lower_lever = math.floor(lever)\n",
    "    upper_lever = math.ceil(lever)\n",
    "\n",
    "    # Get the decimal part\n",
    "    decimal_lever = lever % 1\n",
    "\n",
    "    # Select the data for lower and upper lever boundary\n",
    "    data_lower_lever = data[(data[lever_column] == lower_lever)].reset_index()\n",
    "    data_upper_lever = data[(data[lever_column] == upper_lever)].reset_index()\n",
    "\n",
    "    # Calculate the values according to the given lever\n",
    "    column_mean_lever = data_lower_lever[column] * (1-decimal_lever) + data_upper_lever[column] * decimal_lever\n",
    "\n",
    "    # Create a Dataframe with the new lever. Hereby we take as basis a DataFrame with only one lever selected (here the lower lever)\n",
    "    # which has the same indices and columns as the newly calculated lever. Then we change the lever value and the interpolated values\n",
    "    data_mean_lever = data_lower_lever.copy()\n",
    "    data_mean_lever = data_mean_lever.drop(columns=['index'])\n",
    "    data_mean_lever[column] = column_mean_lever\n",
    "    data_mean_lever[lever_column] = lever\n",
    "\n",
    "    return data_mean_lever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41fbf1",
   "metadata": {},
   "source": [
    "#### 2.2 Calculate Population Data for Life Scenario\n",
    "\n",
    "For the Life scenario we have a lever of __1.5__ for the population, so we calculate the needed values with the function defined above. Furthemore we calculate the share of each age group and bring together historical and future data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd0896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate population for lever 1.5 and aggregate the age groups together\n",
    "pop_lever_1_5 = calculate_new_lever(pop_life, 'population', 1.5)\n",
    "\n",
    "# Calculate the share of each age group\n",
    "pop_lever_1_5['share age group'] = pop_lever_1_5.apply(lambda x: x['population'] / pop_lever_1_5[(pop_lever_1_5['Country'] == x['Country']) & (pop_lever_1_5['Years'] == x['Years'])]['population'].sum(), axis=1)\n",
    "\n",
    "# Get total population from historic and future populations by creating a new dataframe\n",
    "pop_tot = pd.concat([pop_hist, pop_lever_1_5])\n",
    "\n",
    "# Calculate the Population of the EU\n",
    "pop_tot_EU = pop_tot.groupby('Years').agg({'population':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df73187",
   "metadata": {},
   "source": [
    "### 3. Process Data for each country\n",
    "\n",
    "#### 3.1 Define needed Values\n",
    "We define the needed values, as provided in the Google Docs document. Therefore we make use of a dictionary, so we can define for each wanted variable needed parameters. There are parameters used for all variables, and some depend it the Boolean __from_scenario__ is True or False. \n",
    "\n",
    "When the Booleans is True, the data was taken from the EUcalc Website and is available as one csv-file per country and already processed to meet the correct lever for the Life scenario (folder __country_data__). Otherwise the data is from the Bitbucket repo and processed to use the correct lever (folder __data__).\n",
    "\n",
    "\n",
    "General Parameters are:\n",
    "- __from_scenario__: _Boolean_; Defines if the Data for that variable comes from the scenario specific data (True) or is calculated from the data from the EUcalc repository (False)\n",
    "- __filename__: _String_; Defines the filename in which the data for that variable can be found\n",
    "- __unit__: _String_; The unit for that variable, is used to make the resulting DataFrame more viewable\n",
    "- __per_day__: _Boolean_; Defines if the Data from EUcalc is defines as 'perDay' and has to be processed to get the annual data\n",
    "\n",
    "Parameters when __from_scenario__ == True:\n",
    "- __column__: _Boolean_; The column needed for that variable. Is used to access the correct data. If 'sum' is defined, the sum of all columns is used.\n",
    "\n",
    "Parameters when __from_scenario__ == False:\n",
    "- __lever__: _String_; Defines the name of the lever\n",
    "- __age__: _Boolean_; Define if for that variable the data is available as per Age Groups Data. If yes, it is processed to get the total data.\n",
    "- __data__: _Array_; Defines the columns used to calculate the needed value. Hereby the sum of all columns will be taken to calculate the needed values for the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c420995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All needed lever values\n",
    "needed_values_dict = {\n",
    "    'final energy demand': \n",
    "    {\n",
    "        'from_scenario': True,\n",
    "        'filename': 'effective-energy-demand',\n",
    "        'unit': 'TWh',\n",
    "        'per_day':False,\n",
    "        'column': 'sum'\n",
    "    },\n",
    "    'final energy demand industry': {\n",
    "        'from_scenario': True,\n",
    "        'filename': 'effective-energy-demand',\n",
    "        'unit': 'TWh',\n",
    "        'per_day':False,\n",
    "        'column': 'Industry'\n",
    "    },\n",
    "    'living space':\n",
    "    {\n",
    "        'lever':'lever_floor-intensity',\n",
    "        'filename': 'fts_lfs_floor-intensity.csv',\n",
    "        'filename_hist': 'ots_lfs_floor-intensity.csv',\n",
    "        'from_scenario': False,\n",
    "        'age': True,\n",
    "        'per_day': False,\n",
    "        'unit': 'm2',\n",
    "        'data': ['fts_lfs_floor-intensity_space-cap[m2/cap]'] \n",
    "    },\n",
    "    'floor area in commercial and public buildings':\n",
    "    {\n",
    "        'lever':'lever_nonres-floorarea',\n",
    "        'filename': 'fts_bld_nonres-floorarea.csv',\n",
    "        'filename_hist': 'ots_bld_nonres-floorarea.csv',\n",
    "        'lever_value':4,\n",
    "        'from_scenario': False,\n",
    "        'age': False,\n",
    "        'per_day': False,\n",
    "        'unit': '1000m2',\n",
    "        'data': ['fts_bld_nonres-floorarea_offices[1000m2]','fts_bld_nonres-floorarea_hotels[1000m2]','fts_bld_nonres-floorarea_trade[1000m2]','fts_bld_nonres-floorarea_education[1000m2]','fts_bld_nonres-floorarea_health[1000m2]', 'fts_bld_nonres-floorarea_other[1000m2]'] \n",
    "    },\n",
    "    'average distance travelled':\n",
    "    {   \n",
    "        'lever':'lever_pkm',\n",
    "        'filename': 'fts_lfs_pkm.csv',\n",
    "        'filename_hist': 'ots_lfs_pkm.csv',\n",
    "        'from_scenario': False,\n",
    "        'age': True,\n",
    "        'per_day': False,\n",
    "        'unit': 'pkm',\n",
    "        'data': ['fts_lfs_pkm_pkm[pkm/cap]']\n",
    "        \n",
    "    },\n",
    "    'average distance travelled car':\n",
    "    {\n",
    "        'from_scenario': True,\n",
    "        'filename': 'passenger-distance-per-m',\n",
    "        'unit': 'pkm',\n",
    "        'per_day':False,\n",
    "        'column': 'Cars'\n",
    "    },\n",
    "    'average distance travelled plane': \n",
    "    {\n",
    "        'from_scenario': True,\n",
    "        'filename': 'passenger-distance-per-m',\n",
    "        'unit': 'pkm',\n",
    "        'per_day':False,\n",
    "        'column': 'Aviation'\n",
    "    },\n",
    "    'transported goods':\n",
    "    {\n",
    "        'lever': 'lever_freight_tkm',\n",
    "        'filename': 'fts_tra_freight_tkm.csv',\n",
    "        'filename_hist': 'ots_tra_freight_tkm.csv',\n",
    "        'from_scenario': False,\n",
    "        'age': False,\n",
    "        'per_day': False,\n",
    "        'unit': 'km',\n",
    "        'data': ['fts_tra_freight_tkm_lastmile-total-demand[bn_tkm]', 'fts_tra_freight_tkm_longdistance-total-demand[bn_tkm]']\n",
    "    },\n",
    "    'meat consumption':\n",
    "    {\n",
    "        'lever':'lever_diet',\n",
    "        'filename': 'fts_lfs_diet.csv',\n",
    "        'filename_hist': 'ots_lfs_diet.csv',\n",
    "        'from_scenario': False,\n",
    "        'age':True,\n",
    "        'per_day': True,\n",
    "        'unit': 'kcal',\n",
    "        'data': ['fts_lfs_diet_bov[kcal/cap/day]','fts_lfs_diet_sheep[kcal/cap/day]','fts_lfs_diet_pigs[kcal/cap/day]','fts_lfs_diet_poultry[kcal/cap/day]','fts_lfs_diet_oth-animals[kcal/cap/day]']\n",
    "    },\n",
    "    'food waste':\n",
    "    {\n",
    "        'lever':'lever_fwaste',\n",
    "        'filename': 'fts_lfs_fwaste.csv',\n",
    "        'filename_hist': 'ots_lfs_fwaste.csv',\n",
    "        'from_scenario': False,\n",
    "        'age': True,\n",
    "        'per_day': True,\n",
    "        'unit': 'kcal',\n",
    "        'data': ['fts_lfs_fwaste_wine[kcal/cap/day]','fts_lfs_fwaste_beer[kcal/cap/day]','fts_lfs_fwaste_bev-fer[kcal/cap/day]','fts_lfs_fwaste_bev-alc[kcal/cap/day]','fts_lfs_fwaste_cereals[kcal/cap/day]','fts_lfs_fwaste_fruits[kcal/cap/day]','fts_lfs_fwaste_oilcrops[kcal/cap/day]','fts_lfs_fwaste_pulses[kcal/cap/day]','fts_lfs_fwaste_starch[kcal/cap/day]','fts_lfs_fwaste_coffee[kcal/cap/day]','fts_lfs_fwaste_stm[kcal/cap/day]','fts_lfs_fwaste_sugar[kcal/cap/day]','fts_lfs_fwaste_sweet[kcal/cap/day]','fts_lfs_fwaste_voil[kcal/cap/day]','fts_lfs_fwaste_veg[kcal/cap/day]','fts_lfs_fwaste_dfish[kcal/cap/day]','fts_lfs_fwaste_ffish[kcal/cap/day]','fts_lfs_fwaste_pfish[kcal/cap/day]','fts_lfs_fwaste_seafood[kcal/cap/day]','fts_lfs_fwaste_oth-aq-animals[kcal/cap/day]','fts_lfs_fwaste_egg[kcal/cap/day]','fts_lfs_fwaste_milk[kcal/cap/day]','fts_lfs_fwaste_offal[kcal/cap/day]','fts_lfs_fwaste_bov[kcal/cap/day]','fts_lfs_fwaste_sheep[kcal/cap/day]','fts_lfs_fwaste_pigs[kcal/cap/day]','fts_lfs_fwaste_poultry[kcal/cap/day]','fts_lfs_fwaste_oth-animals[kcal/cap/day]','fts_lfs_fwaste_afats[kcal/cap/day]','fts_lfs_fwaste_rice[kcal/cap/day]']\n",
    "    },\n",
    "    'cement production':\n",
    "    {\n",
    "        'from_scenario': True,\n",
    "        'filename': 'material-production',\n",
    "        'unit': 'Mt',\n",
    "        'per_day':False,\n",
    "        'column': 'Cement'\n",
    "    },\n",
    "    'steel production':\n",
    "    {\n",
    "        'from_scenario': True,\n",
    "        'filename': 'material-production',\n",
    "        'unit': 'Mt',\n",
    "        'per_day':False,\n",
    "        'column': 'Steel'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064b9b78",
   "metadata": {},
   "source": [
    "#### 3.2 Process Repository Data\n",
    "\n",
    "For the Data from the EUcalc repository we need to process it, so we get the correct data for the lever defined in the Life scenario. We do that in the next step, and the create a csv-file per country containing the data of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7499a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the lever_value from the levers dictonary defined for the Life scenario\n",
    "# Then calculates the new values. Depending if its a decimal or not, it used the calculate_new_lever function\n",
    "def get_lever_values(data, lever, columns, age, variable, country, variable_parameters):\n",
    "    '''\n",
    "    Gets the lever_value from the levers dictonary defined for the Life scenario\n",
    "    Then calculates the new values. Depending if its a decimal or not, it used the calculate_new_lever function\n",
    "    :param data: Dataframe with the data\n",
    "    :param lever: Lever name\n",
    "    :param columns: Columns for which the lever should be calculated\n",
    "    :param age: Boolean if the data is available in age groups\n",
    "    :param variable: Variable name\n",
    "    :param country: Country name\n",
    "    :param variable_parameters: Variable parameters\n",
    "    :return: Dataframe with the calculated lever\n",
    "    '''\n",
    "    # Get the lever value for life\n",
    "    if 'lever_value' in variable_parameters.keys():\n",
    "        lever_value = variable_parameters['lever_value']\n",
    "    else:\n",
    "        if lever not in levers_life.index:\n",
    "            return None\n",
    "        \n",
    "        # Get the lever defined for the life scenario\n",
    "        lever_value = levers_life.at[lever, 'lever']\n",
    "\n",
    "    # If the data is available in age groups, get the correct value for all age groups\n",
    "    if age:\n",
    "       # Calculated the weighted aggregration\n",
    "       pop_country = pop_lever_1_5[pop_lever_1_5['Country'] == country]\n",
    "       share_age_group = pop_country[['Age', 'share age group']]\n",
    "       data = data[['Country', 'Years'] + [lever] +  ['Age'] + columns]\n",
    "       data['share age'] = data.apply(lambda x: share_age_group[share_age_group['Age'] == x['Age']]['share age group'].values[0], axis=1) \n",
    "       data[columns] = data[columns].multiply(data['share age'], axis='index')\n",
    "       data = data.drop(columns=['share age'])\n",
    "       data = data.groupby(['Country', 'Years', f'{lever}'], as_index =False).sum()\n",
    "\n",
    "    try:\n",
    "        # Check if Lever is a clean value, than we can directly get the data\n",
    "        if lever_value % 1 == 0:\n",
    "            data_lever = data[(data[lever] == lever_value) & (data['Country'] == country)]\n",
    "            data_lever[variable] = data_lever[columns].sum(axis=1, numeric_only=True)\n",
    "            data_lever = data_lever[['Country', 'Years'] + [lever] + [variable]]\n",
    "            return data_lever\n",
    "    \n",
    "        # Otherwise calculate the data based on the lever \n",
    "        else:\n",
    "            data = data[data['Country'] == country]\n",
    "            data_lever = data[['Years', 'Country']].copy()\n",
    "            for column in columns:\n",
    "                data_column_lever = calculate_new_lever(data, column, lever_value, lever_column=lever)\n",
    "                data_lever[column] = data_column_lever[column]\n",
    "            data_lever[variable] = data_lever[columns].sum(axis=1, numeric_only=True)\n",
    "            data_lever = data_lever[['Country', 'Years'] + [lever] + [variable]]\n",
    "            return data_lever\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d05426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanni\\AppData\\Local\\Temp\\ipykernel_5792\\1260999407.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['share age'] = data.apply(lambda x: share_age_group[share_age_group['Age'] == x['Age']]['share age group'].values[0], axis=1)\n",
      "C:\\Users\\yanni\\AppData\\Local\\Temp\\ipykernel_5792\\1260999407.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[columns] = data[columns].multiply(data['share age'], axis='index')\n",
      "C:\\Users\\yanni\\AppData\\Local\\Temp\\ipykernel_5792\\1260999407.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_lever[variable] = data_lever[columns].sum(axis=1, numeric_only=True)\n",
      "C:\\Users\\yanni\\AppData\\Local\\Temp\\ipykernel_5792\\1260999407.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_lever[variable] = data_lever[columns].sum(axis=1, numeric_only=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yanni\\Documents\\09_Job\\HiWi\\02_Projekte\\sufficiency\\main.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     data_lever_initial \u001b[39m=\u001b[39m get_lever_values(data_file, variable_parameters[\u001b[39m'\u001b[39m\u001b[39mlever\u001b[39m\u001b[39m'\u001b[39m], variable_parameters[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], variable_parameters[\u001b[39m'\u001b[39m\u001b[39mage\u001b[39m\u001b[39m'\u001b[39m], key, country, variable_parameters)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m# Calculate the values of the variable\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     data_lever \u001b[39m=\u001b[39m get_lever_values(data_file, variable_parameters[\u001b[39m'\u001b[39;49m\u001b[39mlever\u001b[39;49m\u001b[39m'\u001b[39;49m], variable_parameters[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m], variable_parameters[\u001b[39m'\u001b[39;49m\u001b[39mage\u001b[39;49m\u001b[39m'\u001b[39;49m], key, country, variable_parameters)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39m# Check if the Results are valid\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mif\u001b[39;00m data_lever \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Merge the Initial and new Data together\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\yanni\\Documents\\09_Job\\HiWi\\02_Projekte\\sufficiency\\main.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m share_age_group \u001b[39m=\u001b[39m pop_country[[\u001b[39m'\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mshare age group\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m data \u001b[39m=\u001b[39m data[[\u001b[39m'\u001b[39m\u001b[39mCountry\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mYears\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m [lever] \u001b[39m+\u001b[39m  [\u001b[39m'\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m columns]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mshare age\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: share_age_group[share_age_group[\u001b[39m'\u001b[39;49m\u001b[39mAge\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m x[\u001b[39m'\u001b[39;49m\u001b[39mAge\u001b[39;49m\u001b[39m'\u001b[39;49m]][\u001b[39m'\u001b[39;49m\u001b[39mshare age group\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues[\u001b[39m0\u001b[39;49m], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m data[columns] \u001b[39m=\u001b[39m data[columns]\u001b[39m.\u001b[39mmultiply(data[\u001b[39m'\u001b[39m\u001b[39mshare age\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mshare age\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\frame.py:10037\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10025\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10027\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m  10028\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m  10029\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10035\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m  10036\u001b[0m )\n\u001b[1;32m> 10037\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\apply.py:831\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    829\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 831\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\apply.py:957\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 957\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    959\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    960\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\apply.py:973\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    971\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    972\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 973\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(v, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m    974\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    975\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    976\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    977\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\yanni\\Documents\\09_Job\\HiWi\\02_Projekte\\sufficiency\\main.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m share_age_group \u001b[39m=\u001b[39m pop_country[[\u001b[39m'\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mshare age group\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m data \u001b[39m=\u001b[39m data[[\u001b[39m'\u001b[39m\u001b[39mCountry\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mYears\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m [lever] \u001b[39m+\u001b[39m  [\u001b[39m'\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m columns]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mshare age\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: share_age_group[share_age_group[\u001b[39m'\u001b[39;49m\u001b[39mAge\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m x[\u001b[39m'\u001b[39;49m\u001b[39mAge\u001b[39;49m\u001b[39m'\u001b[39;49m]][\u001b[39m'\u001b[39m\u001b[39mshare age group\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m data[columns] \u001b[39m=\u001b[39m data[columns]\u001b[39m.\u001b[39mmultiply(data[\u001b[39m'\u001b[39m\u001b[39mshare age\u001b[39m\u001b[39m'\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yanni/Documents/09_Job/HiWi/02_Projekte/sufficiency/main.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mshare age\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\frame.py:3887\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3885\u001b[0m \u001b[39m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3886\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3887\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_bool_array(key)\n\u001b[0;32m   3889\u001b[0m \u001b[39m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3890\u001b[0m \u001b[39m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3891\u001b[0m is_single_key \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\frame.py:3949\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3946\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   3948\u001b[0m indexer \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[1;32m-> 3949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_with_is_copy(indexer, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\generic.py:4088\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4077\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m   4078\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_take_with_is_copy\u001b[39m(\u001b[39mself\u001b[39m, indices, axis: Axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[0;32m   4079\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4080\u001b[0m \u001b[39m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4081\u001b[0m \u001b[39m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4086\u001b[0m \u001b[39m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4087\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4088\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   4089\u001b[0m     \u001b[39m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4090\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39m_get_axis(axis)\u001b[39m.\u001b[39mequals(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\generic.py:4068\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4063\u001b[0m     \u001b[39m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4064\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\n\u001b[0;32m   4065\u001b[0m         indices\u001b[39m.\u001b[39mstart, indices\u001b[39m.\u001b[39mstop, indices\u001b[39m.\u001b[39mstep, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp\n\u001b[0;32m   4066\u001b[0m     )\n\u001b[1;32m-> 4068\u001b[0m new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   4069\u001b[0m     indices,\n\u001b[0;32m   4070\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_block_manager_axis(axis),\n\u001b[0;32m   4071\u001b[0m     verify\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   4072\u001b[0m )\n\u001b[0;32m   4073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[39m=\u001b[39mnew_data\u001b[39m.\u001b[39maxes)\u001b[39m.\u001b[39m__finalize__(\n\u001b[0;32m   4074\u001b[0m     \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtake\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4075\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\internals\\managers.py:876\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    873\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[axis]\n\u001b[0;32m    874\u001b[0m indexer \u001b[39m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[39m=\u001b[39mverify)\n\u001b[1;32m--> 876\u001b[0m new_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxes[axis]\u001b[39m.\u001b[39;49mtake(indexer)\n\u001b[0;32m    877\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreindex_indexer(\n\u001b[0;32m    878\u001b[0m     new_axis\u001b[39m=\u001b[39mnew_labels,\n\u001b[0;32m    879\u001b[0m     indexer\u001b[39m=\u001b[39mindexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    882\u001b[0m     copy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    883\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\indexes\\base.py:1158\u001b[0m, in \u001b[0;36mIndex.take\u001b[1;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[0;32m   1157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m-> 1158\u001b[0m     taken \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake(\n\u001b[0;32m   1159\u001b[0m         values, indices, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_na_value\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[0;32m   1161\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[39m# algos.take passes 'axis' keyword which not all EAs accept\u001b[39;00m\n\u001b[0;32m   1163\u001b[0m     taken \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mtake(\n\u001b[0;32m   1164\u001b[0m         indices, allow_fill\u001b[39m=\u001b[39mallow_fill, fill_value\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_na_value\n\u001b[0;32m   1165\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\yanni\\anaconda3\\envs\\inatech\\lib\\site-packages\\pandas\\core\\algorithms.py:1317\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[0;32m   1312\u001b[0m     result \u001b[39m=\u001b[39m take_nd(\n\u001b[0;32m   1313\u001b[0m         arr, indices, axis\u001b[39m=\u001b[39maxis, allow_fill\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fill_value\u001b[39m=\u001b[39mfill_value\n\u001b[0;32m   1314\u001b[0m     )\n\u001b[0;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[39m# NumPy style\u001b[39;00m\n\u001b[1;32m-> 1317\u001b[0m     result \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39;49mtake(indices, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1318\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over all countries to get a csv-file per country containing the wanted variables\n",
    "# WARNING, Takes ca. 12 minutes. Can be skipped if all data is already in the output/country_data folder.\n",
    "for country in countries:\n",
    "\n",
    "    # Needed counter to take the first Data as initial Dataframe, to be able to concentate\n",
    "    counter = 0\n",
    "\n",
    "    # Iterate over all needed values defined in the dictionary above\n",
    "    for key in needed_values_dict.keys():\n",
    "\n",
    "        # Get the parameters of that variable\n",
    "        variable_parameters = needed_values_dict[key]\n",
    "\n",
    "        # Check where the Data comes from and only process the variables where the Data comes from the Repository\n",
    "        if variable_parameters['from_scenario'] == False:\n",
    "\n",
    "            # Get the filename\n",
    "            filename = variable_parameters['filename']\n",
    "\n",
    "            # Load the data\n",
    "            data_file = pd.read_csv(f'input/eu_calc_repo_data/{filename}')\n",
    "\n",
    "\n",
    "            # Check for the counter and if its the first variable, use it as initial DataFrame\n",
    "            if counter == 0:\n",
    "                counter = 1\n",
    "\n",
    "                # Calculate the values of the variable\n",
    "                data_lever_initial = get_lever_values(data_file, variable_parameters['lever'], variable_parameters['data'], variable_parameters['age'], key, country, variable_parameters)\n",
    "            else:\n",
    "                # Calculate the values of the variable\n",
    "                data_lever = get_lever_values(data_file, variable_parameters['lever'], variable_parameters['data'], variable_parameters['age'], key, country, variable_parameters)\n",
    "                \n",
    "                # Check if the Results are valid\n",
    "                if data_lever is not None:\n",
    "\n",
    "                    # Merge the Initial and new Data together\n",
    "                    data_lever_initial = pd.merge(\n",
    "                        left=data_lever_initial,\n",
    "                        on=['Years', 'Country'],\n",
    "                        right=data_lever,\n",
    "                        how='left'\n",
    "                    )\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Export Data as csv\n",
    "    data_lever_initial.to_csv(f'output/country_data/{country}_life_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974ebf1b",
   "metadata": {},
   "source": [
    "#### 3.3 Create Long DataFrame\n",
    "\n",
    "Now we put all needed variables together into one DataFrame with the columns 'Year', 'Country', 'Variable name', 'Variable total', 'Variable per capita'. Therefore we use the csv-files created in the step before, as well as the perCountry files for the variables with __from_scenario__ == True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b399c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanni\\AppData\\Local\\Temp\\ipykernel_5792\\1654709884.py:93: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  eu_calc_long_dataframe = pd.concat([eu_calc_long_dataframe, data_variable_country_df])\n",
      "C:\\Users\\yanni\\AppData\\Local\\Temp\\ipykernel_5792\\1654709884.py:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eu_calc_long_dataframe[eu_calc_long_dataframe['Year'] > 2015]['Historical'] = 1\n"
     ]
    }
   ],
   "source": [
    "# Define a initial Dataframe \n",
    "eu_calc_long_dataframe = pd.DataFrame([], columns=['Year', 'Country', 'Variable name', 'Variable total', 'Variable per capita'])\n",
    "\n",
    "# Iterate over the countries\n",
    "for country in countries:\n",
    "\n",
    "    # Load the csv-file created in the step before\n",
    "    data_country = pd.read_csv(f'output/country_data/{country}_life_data.csv')\n",
    "\n",
    "    # Get the Population for the country\n",
    "    pop_country_abs = pop_tot[pop_tot['Country'] == country].groupby(['Years'], as_index=False).agg({'population':'sum'})\n",
    "    \n",
    "    # Get the years\n",
    "    years = pop_country_abs['Years'].unique()\n",
    "\n",
    "    # Iterate over the needed values\n",
    "    for key in needed_values_dict.keys():\n",
    "\n",
    "        # Get the parameters for that variable\n",
    "        variable_params = needed_values_dict[key]\n",
    "\n",
    "        # Check for the from_scenario Bool\n",
    "        if variable_params['from_scenario'] == False:\n",
    "\n",
    "            # load historic data \n",
    "            data_variable_hist =  pd.read_csv(f'input/eu_calc_repo_data/{variable_params[\"filename_hist\"]}')\n",
    "\n",
    "            # Select country specific data, where only years below 2020 are accounted\n",
    "            data_variable_hist = data_variable_hist[(data_variable_hist['Country'] == country) & (data_variable_hist['Years'] < 2020)]\n",
    "\n",
    "            # rename columns so they are named in the way as they're defined in the needed_values_dict\n",
    "            columns = [x.replace('fts_', '') if x.replace('fts_', '') in data_variable_hist.columns else x.replace('fts', 'ots') for x in variable_params[\"data\"]]\n",
    "            \n",
    "            # sum up all the needed data defined in the needed_values_dict\n",
    "            data_variable_hist = data_variable_hist[columns].sum(axis=1, numeric_only=True)\n",
    " \n",
    "            # If False, the data can be directly accessed in the country specific csv file created before.\n",
    "            if key in data_country.columns:\n",
    "                data_variable = data_country[key]\n",
    "                data_variable = pd.concat([data_variable_hist, data_variable])\n",
    "\n",
    "                # Check if data needs to be processed to annual data\n",
    "                if variable_params['per_day']:\n",
    "                    data_variable = data_variable * 365\n",
    "\n",
    "                # Check if the data is age specific. If yes, it is also always given as perCapita\n",
    "                if variable_params['age']:\n",
    "                    \n",
    "                    # Calculate the absolute and per Capita data\n",
    "                    data_variable_capita = data_variable.values\n",
    "                    data_variable_abs = data_variable.values * pop_country_abs['population']\n",
    "\n",
    "                else:\n",
    "                    # Calculate the absolute and per Capita data\n",
    "                    data_variable_capita = data_variable.values / pop_country_abs['population']\n",
    "                    data_variable_abs = data_variable.values\n",
    "            else:\n",
    "                print('Error, key not in Life data')\n",
    "                print(variable_params[\"lever\"])\n",
    "            \n",
    "        else:\n",
    "            # If from_scenario == True, load the correct csv-file\n",
    "            data_variable = pd.read_csv(f'input/country_data/{variable_params[\"filename\"]}_{country}.csv', index_col=[0], sep=\";\", decimal=',')\n",
    "            \n",
    "            # Select only the needed years\n",
    "            data_variable = data_variable.loc[years]\n",
    "\n",
    "            # Check if one column or the sum is wanted\n",
    "            if variable_params['column'] == 'sum':\n",
    "                data_variable = data_variable.sum(axis=1)\n",
    "                \n",
    "                # Calculate the absolute and per Capita data\n",
    "                data_variable_abs = data_variable.values\n",
    "                data_variable_capita = data_variable.values / pop_country_abs['population']\n",
    "            else:\n",
    "                data_variable = data_variable[variable_params['column']]\n",
    "                \n",
    "                # Calculate the absolute and per Capita data\n",
    "                data_variable_abs = data_variable.values\n",
    "                data_variable_capita = data_variable.values / pop_country_abs['population']\n",
    "        \n",
    "        # Create a Dataframe for the Country and the Variable\n",
    "        data_variable_country_df = pd.DataFrame([], columns=['Year', 'Country', 'Variable total', 'Variable per capita'])\n",
    "        data_variable_country_df['Year'] = years\n",
    "        data_variable_country_df['Country'] = country\n",
    "        data_variable_country_df['Variable name'] = f'{key}'\n",
    "        data_variable_country_df['Variable total'] = data_variable_abs\n",
    "        data_variable_country_df['Unit total'] = variable_params[\"unit\"]\n",
    "        data_variable_country_df['Variable per capita'] = data_variable_capita\n",
    "        data_variable_country_df['Unit per capita'] = f'{variable_params[\"unit\"]} / capita'\n",
    "\n",
    "        # Concat all variables together to a long Dataframe\n",
    "        eu_calc_long_dataframe = pd.concat([eu_calc_long_dataframe, data_variable_country_df])\n",
    "\n",
    "# Calculate the EU-Values\n",
    "# Calculate the EU-Values\n",
    "for variable in eu_calc_long_dataframe['Variable name'].unique():\n",
    "    \n",
    "    # Get Sum for that variable (Absolute Value)\n",
    "    eu_data = eu_calc_long_dataframe[eu_calc_long_dataframe['Variable name'] == variable].groupby(['Year'], as_index=False).agg({'Variable total':'sum', 'Unit total':'first', 'Unit per capita':'first'})\n",
    "    \n",
    "    # Define Variable Name and Country\n",
    "    eu_data['Variable name'] = variable\n",
    "    eu_data['Country'] = 'EU'\n",
    "\n",
    "    # Calculate Per Capita Value\n",
    "    years = eu_data['Year'].unique()\n",
    "    pop_eu = pop_tot_EU.loc[years]\n",
    "    eu_data['Variable per capita'] = eu_data['Variable total'].values / pop_eu['population'].values\n",
    "\n",
    "    # Concat all variables together to a long Dataframe\n",
    "    eu_calc_long_dataframe = pd.concat([eu_calc_long_dataframe, eu_data])\n",
    "\n",
    "# Map Historical Bool to data\n",
    "eu_calc_long_dataframe['Historical'] = 1\n",
    "eu_calc_long_dataframe[eu_calc_long_dataframe['Year'] > 2015]['Historical'] = 1\n",
    "\n",
    "# Reindex the Dataframe\n",
    "eu_calc_long_dataframe = eu_calc_long_dataframe.set_index(['Year', 'Country'])\n",
    "\n",
    "# Export as CSV and Excel\n",
    "eu_calc_long_dataframe.to_csv(f'output/variables_countries.csv')\n",
    "eu_calc_long_dataframe.to_excel(f'output/variables_countries.xlsx')\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
